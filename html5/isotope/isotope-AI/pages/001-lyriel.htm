<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="UTF-8">
  <title>AI - Documentation </title>
  <link rel="stylesheet" href="./css/style.css">

</head>
<body>
<!-- partial:index.partial.html -->
<nav id="navbar">
  <header><h1>AI workflows</h1></header>
    <ul>
      <li><a class="nav-link" href="#Introduction">Introduction</a></li>
      <li><a class="nav-link" href="#lyriel">WF 01: Lyriel</a></li>
      <li><a class="nav-link" href="#Amigurumi">WF 02: Amigurumi</a></li>
      <li><a class="nav-link" href="#MMD-CNET">WF 03: MMD-CNET</a></li>
      <li><a class="nav-link" href="#fromPixelart">WF 04: From Pixelart to 3D</a></li>
      <li><a class="nav-link" href="#KyotoForest">WF 05: Kyoto Forest</a></li>
      <li><a class="nav-link" href="#StyleVideos">WF 06: Style Videos</a></li>
      <li><a class="nav-link" href="#masayume">masayume</a></li>
    </ul>
  </nav>


<div id="main-doc" class="main">
  <section id="Introduction" class="main-section">
    <header>AI Workflows</header>
    <ul>
      <li><a href="https://www.reddit.com/r/StableDiffusion/?f=flair_name%3A%22Workflow%20Included%22">Workflow Included on /r/StableDiffusion</a></li>
      <li><a href="https://onlinehtmleditor.dev/" target="_blank">rich text editor online</a></a> </li>
    </ul>
  </section>

  
  <section id="lyriel" class="main-section">
    <header>Lyriel Workflow</header>

    <div>

  <div style="float: left; width: 40%">
    <p>
      <img src="../img/010.webp" style="width: 512px;">
    </p>
  </div>
  <div style="float: right; width: 60%">
    <p><strong>Step 1 -</strong>&nbsp;Generate Base Potrait (txt2img):</p>
<p><strong>Prompt:</strong>&nbsp;(8k, RAW photo, best quality, masterpiece:1.2), (realistic, photo-realistic:1.37), ultra high res, ultra-detailed, woman, professional lighting, photon mapping, radiosity, physically-based rendering, (detailed symmetrical face), (extremely detailed eyes and face), (symmetrical mouth), (symmetrical eyes) , (detailed eyebrows), (extremely detailed eyes), (detailed beautiful lips), (symmetrical lips), light on face, ((chinese clothes)), ((big red eyes)), ((flower hair pin)), ((tight clothes))</p>
<p><strong>Negative prompt:</strong>&nbsp;Paintings, sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, ((monochrome)), ((grayscale)), glans ,extra fingers ,fewer fingers ,strange fingers ,bad hand ,signature, watermark, username, blurry, bad feet ,bad leg, duplicate, extra limb, ugly, disgusting, light reflection, poorly drawn hands, weird teeth showing, abnormal upper tooth, unbalance eyes, thick eyebrows, abnormal smile, abnormal lips ,unsymmetrical lips, artificial glow, unsymmetrical teeth, abnormal teeth, missing limb, glowing, weird shadow, floating limbs, disconnected limbs, malformed hands, blurry ,mutated hands and fingers, loli, young</p>
<p><strong>Steps:</strong>&nbsp;30,&nbsp;<strong>Sampler:</strong>&nbsp;DPM++ 2M Karras,&nbsp;<strong>CFG scale:</strong>&nbsp;7,&nbsp;<strong>Seed:</strong>&nbsp;3188709985,&nbsp;<strong>Size:</strong>&nbsp;512x768,&nbsp;<strong>Model hash:</strong>&nbsp;f1b08b30f8,&nbsp;<strong>Model:</strong>&nbsp;lyriel_v14,&nbsp;<strong>Clip skip:</strong>&nbsp;2,&nbsp;<strong>Restore Faces:</strong>&nbsp;OFF,</p>
<p>------------------------------------------------------------------------------</p>
<p><strong>Step 2</strong>&nbsp;- Outpainting (img2img)</p>
<p>*Maintain the same prompts above</p>
<p><strong>Resize Mode:</strong>&nbsp;Resize and Fill,&nbsp;<strong>Sampling Method:</strong>&nbsp;Euler a / DPM++ SDE Karras / DPM ++ 2M Karras / DDIM (<em>experiment and see which one gives the best result</em>),&nbsp;<strong>Sampling Steps:</strong>&nbsp;50-100,&nbsp;<strong>CFG scale:</strong>&nbsp;7,&nbsp;<strong>Seed:</strong>&nbsp;318870998 ,&nbsp;<strong>Denoising strength:</strong>&nbsp;0.8,&nbsp;<strong>Clip skip:</strong>&nbsp;2,&nbsp;<strong>Script:</strong>&nbsp;Poor man&#39;s outpainting,&nbsp;<strong>Pixel to expand:</strong>&nbsp;128 (<em>you may get weird result if this is set too high per run</em>),&nbsp;<strong>Mask Blur:</strong>&nbsp;4,&nbsp;<strong>Masked Content:</strong>&nbsp;fill,&nbsp;<strong>Outpainting Direction:</strong>&nbsp;down.</p>
<p>------------------------------------------------------------------------------</p>
<p><strong>Step 3</strong>&nbsp;- Inpainting (img2img)</p>
<p>*<em>This step is to fix any artifacts / seam generated from outpainting</em></p>
<p>*Maintain the same prompts above</p>
<p><strong>Resize Mode:</strong>&nbsp;Resize and Fill,&nbsp;<strong>Mask Blur:</strong>&nbsp;4,&nbsp;<strong>Mask Mode:</strong>&nbsp;Inpaint Mask,&nbsp;<strong>Masked Content:</strong>&nbsp;Latent Noise,&nbsp;<strong>Inpaint Area:</strong>&nbsp;Whole Picture,&nbsp;<strong>Sampling Method:</strong>&nbsp;Euler a / DPM++ SDE Karras / DPM ++ 2M Karras / DDIM (<em>experiment and see which one gives the best result</em>),&nbsp;<strong>Sampling Steps:</strong>&nbsp;30-50,&nbsp;<strong>CFG scale:</strong>&nbsp;7,&nbsp;<strong>Seed:</strong>&nbsp;318870998 ,&nbsp;<strong>Denoising strength:</strong>&nbsp;0.8,&nbsp;<strong>Clip skip:</strong>&nbsp;2,</p>
<p>------------------------------------------------------------------------------</p>
<p><strong>Step 4 -</strong>&nbsp;Repeat steps 2&amp;3</p>
<p>This is basically an iterative process where we generate new content bit by bit and subsequently fix any errors via inpainting. For Step 2, once you have generated sufficient vertical pixels, swap&nbsp;<em><strong>Outpainting Direction</strong></em>&nbsp;to Left and Right.&nbsp;<strong>Add or remove prompts depending on the content you want generated.</strong>&nbsp;For the image above, I repeated until it reached 1920px vertically.</p>
<p>------------------------------------------------------------------------------</p>
<p><strong>Step 5 -</strong>&nbsp;Upscale (img2img)</p>
<p>* This is to upscale the image to 4K or higher.</p>
<p><strong>Steps:</strong>&nbsp;30,&nbsp;<strong>Sampler:</strong>&nbsp;DPM++ 2M Karras,&nbsp;<strong>CFG scale:</strong>&nbsp;7,&nbsp;<strong>Seed:</strong>&nbsp;3188709985,&nbsp;<strong>Size:</strong>&nbsp;1088x1920,&nbsp;<strong>Script:</strong>&nbsp;SD Upscale,&nbsp;<strong>Model hash:</strong>&nbsp;4d91c4c217,&nbsp;<strong>Model:</strong>&nbsp;lyriel_v15,&nbsp;<strong>Denoising strength:</strong>&nbsp;0.15,&nbsp;<strong>Clip skip:</strong>&nbsp;2,&nbsp;<strong>SD upscale overlap:</strong>&nbsp;64,&nbsp;<strong>SD upscale upscaler:</strong>&nbsp;lollypop (4x-UltraSharp and Remacri works well too),&nbsp;<strong>Scale Factor:</strong>&nbsp;2</p>
  </div>

</div>
    <ul>
      <li><a href="https://www.reddit.com/r/StableDiffusion/comments/12vemzw/lyriel_outpainting/">original post</a> </li>
    </ul>

    <p>Directories</p>
    <ul>
      <li>A: <code>---</code></li>
      <li>B: <code>---</code></li>
    </ul>
    
  </section>

<br clear="all">

  <section id="Amigurumi" class="main-section">
    <header>Amigurumi Workflow</header>

    <div>

      <div style="float: left; width: 40%">
        <p>
          <img src="../img/020.webp" style="width: 512px;">
        </p>
      </div>
      <div style="float: right; width: 60%">
        <p><strong>Workflow</strong></p>
        <p><strong>Model: SDXL</strong></p>
        <p><strong>Image Generator:</strong> <a href="https://clipdrop.co/stable-diffusion">clipdrop.co/stable-diffusion</a> </p>
        <p><strong>Prompt:</strong> "[Name of World Leader] Amigurumi" (Photographic Style)</p>
      </div>

    </div>

    <br clear="all">

    <ul>
      <li>
        <a href="https://www.reddit.com/r/StableDiffusion/comments/12w2d5q/amigurumi_dolls_of_world_leaders/">original post</a> 
        by <a href="https://www.reddit.com/user/hardmaru/">hardmaru</a> 
      </li>
    </ul>

    <p>Directories</p>
    <ul>
      <li>A: <code>---</code></li>
      <li>B: <code>---</code></li>
    </ul>
    
  </section>

<br clear="all">

  <section id="MMD-CNET" class="main-section">
    <header>MMD - CNET - Lineart Workflow</header>

    <div>

      <div style="float: left; width: 40%">
        <p>
          <img src="../img/030.gif" style="width: 512px;">
        </p>
      </div>
      <div style="float: right; width: 60%">
        <p><strong>Image Generator:</strong> <a href="https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart">controlnet 1.1 lineart</a> </p>
        <p><strong>Workflow</strong></p>
          <p>I installed the new ControlNet 1.1 and used LINART to create the AI animation.</p>
          <p>I am turning MMD into ANIME . Source：【MMD Redshift】Love Trial </p>
          <p>I feel like I can do better with this than the others. </p>
          <p>I am also testing to improve the Ghibli style that I was doing before.</p>
      </div>

    </div>

    <br clear="all">

    <ul>
      <li>
        <a href="https://www.reddit.com/r/StableDiffusion/comments/12uad6a/controlnet11_lineart_test_ai_animation/">original post</a> 
        by <a href="https://www.reddit.com/user/aimikummd/">aimikummd</a> 
      </li>
    </ul>

    <p>Directories</p>
    <ul>
      <li>A: <code>---</code></li>
      <li>B: <code>---</code></li>
    </ul>
    
  </section>

  <br clear="all">

  <section id="fromPixelart" class="main-section">
    <header>From Pixelart to 3D</header>

    <div>

      <div style="float: left; width: 40%">
        <p>
          <img src="../img/040.webp" style="width: 320px;">
        </p>
      </div>
      <div style="float: right; width: 60%">
        <p><strong>Image Generator:</strong> <a href="https://www.reddit.com/r/StableDiffusion/comments/12vk4wk/comment/jhbij1k/?utm_source=reddit&utm_medium=web2x&context=3">ryanrybot workflow</a> </p>
        <p><strong>Workflow</strong></p>
        <p>For all the renders, I loaded the original portraits into img2img and would render each image at a low denoising strength; between 0.4 and 0.55. The initial images would keep much of the pixel look, but the facial features would start to get more detailed. I would then feed that image back into img2img, and refine again. I would repeat this process at least 6 or 7 times.</p>
        <p>If i liked the look of an image, but didn&#39;t like the way certain features were coming out, I would load it up into Photoshop and fix it manually, then feed that image back through img2img with a low (0.25 - 0.4) denoising strength.</p>        
        <p>The final image was then taken into Photoshop for final tweaks and that&#39;s about it.</p>
        <p>Hope that helps anyone trying something similar.</p>
        <p><em><strong>EDIT 1:</strong></em></p>
        <p>Sorry! my denoising had wrong decimal places. Fixed now.</p>
        <p>I used&nbsp;<a href="https://civitai.com/models/10028/neverending-dream-ned" rel="noopener nofollow ugc" target="_blank">NeverEnding Dream</a>,&nbsp;<a href="https://civitai.com/models/6424/chilloutmix" rel="noopener nofollow ugc" target="_blank">ChilloutMix</a>, and&nbsp;<a href="https://civitai.com/models/8281/perfect-world" rel="noopener nofollow ugc" target="_blank">PerfectWorld</a>&nbsp;(NSFW!!)</p>
        <p>My prompts are nothing special, but here&#39;s an example of what I used. I would sometimes add extra prompts between iterations if I felt it needed more or less of something.</p>
        <p><strong>Positive prompt:</strong><br />
        unparalleled masterpiece, ultra realistic, 8k, perfect artwork, ((perfect male figure)), mature man, looking at viewer, alluring, clean, ((shiny skin)), intricate detail, prestige, anime-styled black hair, spiked hair, pale, emo, goth, asymmetrical hair, dark eyes, (lips closed), black hooded sweatshirt, pull strings</p>
        <p><strong>Negative Prompt:</strong><br />
        (worst quality:1.2), (low quality:1.2), (lowres:1.1), (monochrome:1.1), (greyscale),deformed, bad anatomy, disfigured, poorly drawn face, mutation, mutated, ugly, disgusting, missing limb, floating limbs, disconnected limbs, blurry, doubled face, mutated hands, mutated fingers, multiple eyebrows, multiple views, sketch, child face, woman, girl, (((female))), (chiseled jaw)</p>
        <p>Here&#39;s what Penny looked like between each iteration for a clear example of what to expect.</p>
<!--        <p><a href="https://preview.redd.it/ato8vpyhomva1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c1a102eac696372bf47b0b30819b6bf0ef65cc01" rel="noopener noreferrer" target="_blank"><img alt="Comment image" src="https://preview.redd.it/ato8vpyhomva1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=970a45a34158e3dc00cddeab6c828b346ed3ca35" /></a></p> -->
      </div>

    </div>


    <br clear="all">

    <section id="KyotoForest" class="main-section">
      <header>Kyoto Forest</header>
  
      <div>
  
        <div style="float: left; width: 40%">
          <p>
            <img src="../img/050.png" style="width: 512px;">
          </p>
        </div>
        <div style="float: right; width: 60%">

          <p>This model produces images of a red, samurai-esque Japanese forest with a blue river surrounding a Minka house (Japanese, traditional house). There&rsquo;s a ton of variation that can be applied in regards to features of a typical forest such as rocks, trees, clouds, and amounts of grass.</p>

          <p>I&rsquo;ve always enjoyed nature, and where I live; our forests can change colours quite drastically, so I decided to create a LoRa of it. Forests in nature can be incredibly peaceful environments, capturing the sounds of rustling plants, flowing streams, and tiny cracks of sunlight slipping through the leaves of the trees; all traits that I wanted to capture with this model.</p>
          
          <p>Model is available here:&nbsp;<a href="https://dreamscapeapp.com/models/TIMDc3ea482ee01e81d2f7b54c986124ee" rel="noopener nofollow ugc" target="_blank">https://dreamscapeapp.com/models/TIMDc3ea482ee01e81d2f7b54c986124ee</a></p>
          
          <p>Prompting</p>          
          <p>Use a weight of 1, and the trigger word is &ldquo;forestxyz&rdquo;</p>
          
          <p>Example prompts:</p>          
          <ul>
            <li>
            <p>red forestxyz with red trees, river, red house, sunlight</p>
            </li>
            <li>
            <p>red forestxyz with green trees, reflection, minka, rocks, water nearby, cloudy skies, sunlight</p>
            </li>
          </ul>
          
          <p>Recommended settings:</p>
          <p>CFG = 7</p>
          <p>100-150 steps</p>
          <p>Sampler = Euler A</p>
          <p>Resolution - 512x512</p>
          <p>Training info</p>
          <p>Base model is SD1.5</p>          
          <p>300 epochs with 1250-1500 training steps</p>      
          <p>Swing by our Discord as we&#39;re trying to form an AI community:&nbsp;<a href="https://discord.gg/hbKANADmZa" rel="noopener nofollow ugc" target="_blank">https://discord.gg/hbKANADmZa</a></p>          
          <p>&nbsp;</p>
                    
        </div>
  
      </div>
  
      <ul>
        <li>
          <a href="https://www.reddit.com/r/StableDiffusion/comments/1311xvs/kyoto_forest/">original post</a> 
          by <a href="https://www.reddit.com/user/pkkvu/">pkkvu</a> 
        </li>
      </ul>
  
      <p>Directories</p>
      <ul>
        <li>A: <code>---</code></li>
        <li>B: <code>---</code></li>
      </ul>
  
      <br clear="all">


  <section id="StyleVideos" class="main-section">
    <header>Style Videos</header>

    <div>

      <div style="float: left; width: 40%">
        <p>
          <img src="../img/060.webp" style="width: 512px;">
        </p>
      </div>
      <div style="float: right; width: 60%">
        <p>I&rsquo;ve made&nbsp;<a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL25lamNzdXNlYy5iZWVoaWl2LmNvbS9wL2FpLXN1cHBvcnRlZC1hbmltYXRpb24tY29udGludWVzIiwicG9zdF9pZCI6IjM2MzE2MTZmLTRjNmYtNDllMC04MjJkLWQ1NDZhZTk1NmZjYSIsInB1YmxpY2F0aW9uX2lkIjoiNjQ1NjlkMjgtY2M4Yy00NTdmLThmZTctN2NiYmI3YjlhMWVhIiwidmlzaXRfdG9rZW4iOiIwOWI0NTVkYi00YjRjLTRjYjMtOWJiZC1jY2ZlYTc2ZjQ2ZmUiLCJpYXQiOjE2ODQ3OTE2ODEuODUzLCJpc3MiOiJvcmNoaWQifQ.Gw1F6ZjlbK-QOqTxJr1X2i8hP4SKUMuMiKkUpbkcAmA" rel="noopener noreferrer nofollow" target="_blank">a post</a>&nbsp;showing how to use Runway Gen-1 to apply style on top of existing videos.</p>
        <p>There is another software that offers a similar functionality, called EbSynth. I&rsquo;ve used it recently to make&nbsp;<a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3lvdXR1YmUuY29tL3Nob3J0cy9tdUxIenRFUUUtMD9mZWF0dXJlPXNoYXJlJnV0bV9zb3VyY2U9bmVqY3N1c2VjLmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXN0eWxpemluZy12aWRlb3Mtd2l0aC1haSIsInBvc3RfaWQiOiIzNjMxNjE2Zi00YzZmLTQ5ZTAtODIyZC1kNTQ2YWU5NTZmY2EiLCJwdWJsaWNhdGlvbl9pZCI6IjY0NTY5ZDI4LWNjOGMtNDU3Zi04ZmU3LTdjYmJiN2I5YTFlYSIsInZpc2l0X3Rva2VuIjoiMDliNDU1ZGItNGI0Yy00Y2IzLTliYmQtY2NmZWE3NmY0NmZlIiwiaWF0IjoxNjg0NzkxNjgxLjg1MywiaXNzIjoib3JjaGlkIn0.39Iq6iAbWbbNq2D7SNRr5hRSzQOo8Yl_ZG4-hVPLWqk" rel="noopener noreferrer nofollow" target="_blank">this video</a>&nbsp;and I want to show you the process and break down the differences between Gen-1 and EbSynth.</p>
        <p><strong>Let&rsquo;s start creating!</strong></p>
        <p><em>PS: I added a referral program at the bottom of the newsletter. Refer 1 friend, and I&rsquo;ll send you the book&nbsp;</em><em><strong><a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2JyaWFud3N5a2VzLmd1bXJvYWQuY29tL2wvdHpqanc_dXRtX3NvdXJjZT1uZWpjc3VzZWMuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1uZXdzbGV0dGVyJnV0bV9jYW1wYWlnbj1wdXR0aW5nLWNoYXJhY3RlcnMtaW50by1pbWFnZXMtd2l0aC1haSIsInBvc3RfaWQiOiIzNjMxNjE2Zi00YzZmLTQ5ZTAtODIyZC1kNTQ2YWU5NTZmY2EiLCJwdWJsaWNhdGlvbl9pZCI6IjY0NTY5ZDI4LWNjOGMtNDU3Zi04ZmU3LTdjYmJiN2I5YTFlYSIsInZpc2l0X3Rva2VuIjoiMDliNDU1ZGItNGI0Yy00Y2IzLTliYmQtY2NmZWE3NmY0NmZlIiwiaWF0IjoxNjg0NzkxNjgxLjg1MywiaXNzIjoib3JjaGlkIn0.OZCI0q4o7J4zPMEPJspwWfHedI09UXv70qRpoNPk6rs" rel="noopener noreferrer nofollow" target="_blank">AI Explore: Collaborations 1&nbsp;</a></strong></em><em>for free. If you enjoy these posts, why not share them, right?&nbsp;</em>😁</p>
        <p>The best use of AI for videos at the moment is style transfer. We have original footage to which we apply an alternate style.</p>
        <p>For this to work, we need three elements:</p>
        <ul>
          <li>
          <p>Original video footage,</p>
          </li>
          <li>
          <p>Style frame/image</p>
          </li>
          <li>
          <p>Software to combine them together</p>
          </li>
        </ul>
        <p><strong>Original video footage&nbsp;</strong>can be done by recording ourselves or animating 3D models.</p>
        <p>Both work and you can experiment depending on your use case.</p>
        <p>For this example, I found a free 3d model on&nbsp;<a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5taXhhbW8uY29tLyMvP3BhZ2U9MSZ0eXBlPUNoYXJhY3RlciIsInBvc3RfaWQiOiIzNjMxNjE2Zi00YzZmLTQ5ZTAtODIyZC1kNTQ2YWU5NTZmY2EiLCJwdWJsaWNhdGlvbl9pZCI6IjY0NTY5ZDI4LWNjOGMtNDU3Zi04ZmU3LTdjYmJiN2I5YTFlYSIsInZpc2l0X3Rva2VuIjoiMDliNDU1ZGItNGI0Yy00Y2IzLTliYmQtY2NmZWE3NmY0NmZlIiwiaWF0IjoxNjg0NzkxNjgxLjg1MywiaXNzIjoib3JjaGlkIn0.zPjfPFtNl5JkKPjRuICXTfAY0L7MDNw-mm8QTGNmh94" rel="noopener noreferrer nofollow" target="_blank">Mixamo</a>&nbsp;and chose an animation that would fit the video.</p>
        <p>I created the&nbsp;<strong>Style Frame</strong>&nbsp;in Stable Diffusion. I created a lot of concepts in Midjourney and made the final stylization choices in Stable Diffusion.</p>
        <p>Now we come to the&nbsp;<strong>Software.</strong></p>
        <p>I&rsquo;ve used both EbSynth and Gen-1 in various projects and both work well, if you know their limitations and how to work with them.</p>
        <p><strong>EbSynth</strong>&nbsp;works as a brush. We have a canvas, which is the original video, and then we paint over it with the style frame. This works well when there is not a lot of movement. When the characters appearance is mostly the same, the paint will stay where it is, showing the new picture, as closely to the original as possible.</p>
        <p>When there is movement, which creates new shapes and pixels, the paint will move with it. This can result in distortions and empty space, where there shouldn&#39;t be one.</p>
        <p><strong>Gen-1</strong>&nbsp;on the other hand, follows the movements and shapes of the original video and uses the style frame to create a new scene with the reference. This means the moving characters will keep their movement. However, as the software creates new images (which make the video), they will vary from the original style frame.</p>
        <p>So these are the limitations of the software:</p>
        <p>EbSynth - Retains the style frame style, struggles with excessive movement.</p>
        <p>Gen-1 - Changes the style of the style frame, but movements are clear and cohesive. It also has a green screen option, which isolates the subject quickly.</p>
        <p>For my example, there is not much movement, and I wanted to have a more detailed character, so I went with EbSynth.</p>
        <p><strong>What is the EbSynth workflow?</strong></p>
        <p>Convert the original video into an image sequence. I used After Effects, you can use online converters as well. Put the images into a folder and name them in a sequence (converters usually do this for you already).</p>
        <p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/b090982d-4885-46ba-8376-4a642ca8451f/image.png" /></p>
        <p>Create your style frame and put it in a separate folder. Make sure it has the same name as the corresponding image in your image sequence. In my example it&rsquo;s 0000_0000.</p>
        <p>Also make sure the images have the same resolution. It will not work otherwise.</p>
        <p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/0c259b49-e328-48f4-9026-2b9cb2ecce8c/0000_0000.png" /></p>
        <p>Run EbSynth and connect the two folders. Drag the style reference folder into the<em>&nbsp;&ldquo;keyframe&rdquo;</em>&nbsp;box and the original video image sequence into &ldquo;<em>video&rdquo;&nbsp;</em>box.</p>
        <p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/b6dfb97f-118b-4f9d-8f16-e3ac7256c1fb/image.png" /></p>
        <p>Press Run All and you&rsquo;ll get your newly stylized video.</p>
        <p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/7af97160-ec7a-4c6c-852b-172ab8a56a72/green_screen_base_new__1_.gif" /></p>
        <p>As you can see there are some distortions after the character moves, but the style looks exactly as the style frame.</p>
        <p>Here is a comparison of Gen-1 with the same image using the green screen feature.</p>
        <p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/f8e73530-33f5-4090-9666-5b354eeeb156/ezgif-5-7db41aa417.gif" /></p>
        <p>These tools create some interesting new ways to create.</p>
        <p>I&rsquo;ve been sharing a lot of workflows for visual storytelling and I&rsquo;m putting them all into an e-book, for an expedited learning experience -&nbsp;<strong>A 2D artists guide to AI supported workflows.</strong></p>
        <p>I&rsquo;ve already written chapters on&nbsp;<em>Creating concept images, Brainstorming, Stylization, Animation and Combining AI tools with non AI tools.</em></p>
        <p>If there is a workflow you&rsquo;re curious about, if you have an end product in mind and are not sure how you could support it with AI, hit reply and send me a message! I would love to hear from you and make the e-book even more relevant for you.</p>
        <p>It&rsquo;s an exciting time to be a creator. We&rsquo;re able to create in ways never possible before. Let&rsquo;s explore and embrace them together. 😁</p>        

      </div>

    </div>

      <br clear="all">




      <section id="anchor1" class="main-section">
        <header>Kyoto Forest</header>
    
        <div>
    
          <div style="float: left; width: 40%">
            <p>
              <img src="../img/050.png" style="width: 320px;">
            </p>
          </div>
          <div style="float: right; width: 60%">
  
  <!--
  testo da https://onlinehtmleditor.dev/
  -->
            
          </div>
    
        </div>
  
        <ul>
          <li>
            <a href="https://www.reddit.com/r/StableDiffusion/comments/12vu9i9/i_also_made_characters_from_a_favorite_game/">original post</a> 
            by <a href="https://www.reddit.com/user/mynamesjenelle/">mynamesjenelle</a> 
          </li>
        </ul>
    
        <p>Directories</p>
        <ul>
          <li>A: <code>---</code></li>
          <li>B: <code>---</code></li>
        </ul>
    
        <br clear="all">





        <section id="anchor2" class="main-section">
          <header>Kyoto Forest</header>
      
          <div>
      
            <div style="float: left; width: 40%">
              <p>
                <img src="../img/050.png" style="width: 320px;">
              </p>
            </div>
            <div style="float: right; width: 60%">
    
    <!--
    testo da https://onlinehtmleditor.dev/
    -->
              
            </div>
      
          </div>
    
          <ul>
            <li>
              <a href="https://www.reddit.com/r/StableDiffusion/comments/12vu9i9/i_also_made_characters_from_a_favorite_game/">original post</a> 
              by <a href="https://www.reddit.com/user/mynamesjenelle/">mynamesjenelle</a> 
            </li>
          </ul>
      
          <p>Directories</p>
          <ul>
            <li>A: <code>---</code></li>
            <li>B: <code>---</code></li>
          </ul>
      
    <br clear="all">





<br clear="all">




  <section id="Hello_world" class="main-section">
    <header>Hello world</header>
      <p>To get started with writing JavaScript, open the Scratchpad and write your first "Hello world" JavaScript code:</p>
     <code>function greetMe(yourName) { alert("Hello " + yourName);</code>
      <p>Select the code in the pad and hit Ctrl+R to watch it unfold in your browser!</p>
  </section>

  <section id="masayume" class="main-section">
    <header>masayume</header>
    <p>I am an IT pro, blogger, game developer and pixel artist. I am also intersted in Web Developing.</p>
    <p>If you want to see my works, Visit the below links:</p>
    <ul>
      <li>twitter.com/masayumeP</li>
      <li>github.com/masayume</li>
      <li>www.masayume.it</li>
      <li>pinterest.co.uk/masayume</li>
    </ul>
    <p>page designed by <b>Umar Farooq</b>.</p>
      
  </section>
</div>
<!-- partial -->
  <script src='https://kit.fontawesome.com/a076d05399.js'></script>
</body>
</html>
